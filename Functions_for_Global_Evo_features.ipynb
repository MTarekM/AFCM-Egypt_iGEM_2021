{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Functions_for_Global_Evo_features.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cTLVnsoSCGt"
      },
      "source": [
        "#install TAPE\n",
        "!pip install tape_proteins"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui71QHWOSHwD"
      },
      "source": [
        "#feel free to use your model and tokenizer\n",
        "#Here we set model1 to unirep and model2 to BERTprotein\n",
        "#device = 'cuda:0'\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tape import ProteinBertModel, TAPETokenizer,UniRepModel\n",
        "model = UniRepModel.from_pretrained('babbler-1900')\n",
        "#model.cuda(device=device)\n",
        "tokenizer = TAPETokenizer(vocab='unirep')\n",
        "model2 = ProteinBertModel.from_pretrained('bert-base')\n",
        "tokenizer2 = TAPETokenizer(vocab='iupac')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAd_MiSeSVSa"
      },
      "source": [
        "\n",
        "#return Unirep encoding, you can apply to another models by chaning \"model\" and \"tokenizer from last cell\"\n",
        "#This function returns the averages of hidden states\n",
        "def unirepp(sequence):\n",
        "    sequence=sequence.replace(\"-\",\"\")\n",
        "    token_ids = torch.tensor([tokenizer.encode((sequence))])\n",
        "    output = model(token_ids)\n",
        "    sequence_output = output[0]\n",
        "    t = sequence_output\n",
        "    t_np = sequence_output[0].detach().numpy()#convert to Numpy array\n",
        "    df = pd.DataFrame(t_np) #convert to a dataframe\n",
        "    df.loc['mean1'] = df.mean(axis=0)\n",
        "    df=df.drop(df.index[0:-1])\n",
        "    listmean=pd.Series(df.loc['mean1'].values.tolist())\n",
        "    return  list(listmean)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkiT-VvBSyBp"
      },
      "source": [
        "#concatenate global features to a dataframe df\n",
        "#it depends on your design feel free to change dataframes to direct encodings\n",
        "dfglobal=pd.DataFrame(columns=createList(0, 20))\n",
        "#suppose sequences are in \"Mutant\" Column\n",
        "dfglobal = df.apply(lambda row : unirepp(row['Mutant']), axis = 1)\n",
        "dfglobal1=pd.DataFrame(data=dfglobal.tolist())\n",
        "frames=[dfglobal1,df6]\n",
        "dffinal = pd.concat(frames,axis=1)\n",
        "dffinal=dffinal.dropna()\n",
        "dffinal"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}